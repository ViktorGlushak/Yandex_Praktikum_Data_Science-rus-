{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» c BERT\n",
    "\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**План выполнения проекта:**\n",
    "\n",
    "1. Загрузим и подготовим данные;\n",
    "2. Обучим разные модели;\n",
    "3. Сделаем выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и изучим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим на пропуски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>toxic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total  Percent\n",
       "toxic      0      0.0\n",
       "text       0      0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим на сбалансированность классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898321\n",
       "1    0.101679\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные сильно разбалансированы, негативных твитов только 10%.\n",
    " Нужно это учесть при делении на выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка к обучению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет, берём определенное количество твитов случайным\n",
    " образом с учетом баланса классов, возвращаем отдельно сами твиты и метки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_dataframe(count=None, link_csv='/datasets/toxic_comments.csv'):\n",
    "    df1 = pd.read_csv(link_csv)\n",
    "    print('Размер изначального датасета', df1.shape)\n",
    "    if count:\n",
    "        df_pos = df1[df1['toxic']==0].sample(int(count/2))\n",
    "        df_neg = df1[df1['toxic']==1].sample(int(count/2))\n",
    "        df1 = pd.concat([df_pos, df_neg])\n",
    "        print('Размер подвыборки', df1.shape)\n",
    "    print('10 случайных примеров из датасета', df1.sample(10))\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_labels(df):\n",
    "    sentences = df['text'].values\n",
    "    labels = df['toxic'].values\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы долго не обучать модель берем 2000 сбалансированных твитов.\n",
    " Балансируем только тренировочную выборку. Тест - должен быть чистый, не балансированный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер изначального датасета (159571, 2)\n",
      "Размер подвыборки (1600, 2)\n",
      "10 случайных примеров из датасета                                                      text  toxic\n",
      "17174   \"\\n\\nJanuary 24, 2007\\nYour change  was determ...      0\n",
      "52565         You obviously never attended college, moron      1\n",
      "1651    You won't find any creditable citations becaus...      1\n",
      "139350  Chavez sure isnt a hero. He's classic scum. An...      1\n",
      "129445  \"\\n\\nThere is a country called America. There ...      0\n",
      "157647  It is not off board dispute, because legally W...      0\n",
      "120670           Please check the bottom of my talk page.      0\n",
      "32988   \"\\n\\nJosif Stalin IS one of the three worst cr...      0\n",
      "102782  I think that if you are going to use ther exac...      0\n",
      "140888  Aquabuddha \\n\\nWhy is there no mention of his ...      0\n"
     ]
    }
   ],
   "source": [
    "balanced_df = get_balanced_dataframe(count=1600)\n",
    "sentences_train, labels_train = get_text_labels(balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычтем 1600 тех которые взяли для обучения из общей выборки и возьмем еще 400 случайных не сбалансированных. Для наглядности посмотрим на изначальный размер датасета и после вычитания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157971, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbalanced_df = df[(~df.index.isin(balanced_df.index))]\n",
    "unbalanced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    367\n",
       "1     33\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = unbalanced_df.sample(400)\n",
    "df_valid['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_valid, labels_valid = get_text_labels(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем токенайзер. Приводим к нижнему регистру. И проверяем его работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  \"\n",
      "\n",
      " Asking users to contact you privately \n",
      "\n",
      "Dear editor at 84.75.133.108. You are currently (as you have done in the past) contacting users, in particular administrators, asking them to e-mail you privately about something. I do not know what you wish to talk about but I can assure you that there is probably a better way to accomplish whatever it is that you wish to accomplish. In particular, if you have a legal concern about your personal information on any Wikipedia article, please contact the OTRS team as per Wikipedia:BLPEDIT#Legal_issues. If you are being harassed in some manner, please see Wikipedia:Harassment for advice. If it's a research project, read Wikipedia:Research recruitment. If it's a copyright problem, please follow the directions at Wikipedia:Copyright_violations#Information_for_copyright_owners. I can't really think of too many other problems that are better handled off-wiki and privately. Please be aware of our guidelines against \"\"canvassing\"\", which is wiki-slang for trying to gather support for some particular topic. If people do contact you, please read WP:EMAILPOST to know our stance about the privacy of their emails. Be aware that by posting your e-mail address, you are giving away your private information. \"\"Deleting\"\" it from user's talk pages, as you keep asking of editors, does not not remove that information from public view. Lastly, if it's something entirely different than I've anticipated, there's still almost surely a better way to handle it. Search Wikipedia with wisely-chosen keywords to find it.   \"\n",
      "\n",
      "Tokenized:  ['\"', 'asking', 'users', 'to', 'contact', 'you', 'privately', 'dear', 'editor', 'at', '84', '.', '75', '.', '133', '.', '108', '.', 'you', 'are', 'currently', '(', 'as', 'you', 'have', 'done', 'in', 'the', 'past', ')', 'contact', '##ing', 'users', ',', 'in', 'particular', 'administrators', ',', 'asking', 'them', 'to', 'e', '-', 'mail', 'you', 'privately', 'about', 'something', '.', 'i', 'do', 'not', 'know', 'what', 'you', 'wish', 'to', 'talk', 'about', 'but', 'i', 'can', 'assure', 'you', 'that', 'there', 'is', 'probably', 'a', 'better', 'way', 'to', 'accomplish', 'whatever', 'it', 'is', 'that', 'you', 'wish', 'to', 'accomplish', '.', 'in', 'particular', ',', 'if', 'you', 'have', 'a', 'legal', 'concern', 'about', 'your', 'personal', 'information', 'on', 'any', 'wikipedia', 'article', ',', 'please', 'contact', 'the', 'ot', '##rs', 'team', 'as', 'per', 'wikipedia', ':', 'b', '##lp', '##ed', '##it', '#', 'legal', '_', 'issues', '.', 'if', 'you', 'are', 'being', 'harassed', 'in', 'some', 'manner', ',', 'please', 'see', 'wikipedia', ':', 'harassment', 'for', 'advice', '.', 'if', 'it', \"'\", 's', 'a', 'research', 'project', ',', 'read', 'wikipedia', ':', 'research', 'recruitment', '.', 'if', 'it', \"'\", 's', 'a', 'copyright', 'problem', ',', 'please', 'follow', 'the', 'directions', 'at', 'wikipedia', ':', 'copyright', '_', 'violations', '#', 'information', '_', 'for', '_', 'copyright', '_', 'owners', '.', 'i', 'can', \"'\", 't', 'really', 'think', 'of', 'too', 'many', 'other', 'problems', 'that', 'are', 'better', 'handled', 'off', '-', 'wi', '##ki', 'and', 'privately', '.', 'please', 'be', 'aware', 'of', 'our', 'guidelines', 'against', '\"', '\"', 'canvas', '##sing', '\"', '\"', ',', 'which', 'is', 'wi', '##ki', '-', 'slang', 'for', 'trying', 'to', 'gather', 'support', 'for', 'some', 'particular', 'topic', '.', 'if', 'people', 'do', 'contact', 'you', ',', 'please', 'read', 'w', '##p', ':', 'email', '##post', 'to', 'know', 'our', 'stance', 'about', 'the', 'privacy', 'of', 'their', 'emails', '.', 'be', 'aware', 'that', 'by', 'posting', 'your', 'e', '-', 'mail', 'address', ',', 'you', 'are', 'giving', 'away', 'your', 'private', 'information', '.', '\"', '\"', 'del', '##eti', '##ng', '\"', '\"', 'it', 'from', 'user', \"'\", 's', 'talk', 'pages', ',', 'as', 'you', 'keep', 'asking', 'of', 'editors', ',', 'does', 'not', 'not', 'remove', 'that', 'information', 'from', 'public', 'view', '.', 'lastly', ',', 'if', 'it', \"'\", 's', 'something', 'entirely', 'different', 'than', 'i', \"'\", 've', 'anticipated', ',', 'there', \"'\", 's', 'still', 'almost', 'surely', 'a', 'better', 'way', 'to', 'handle', 'it', '.', 'search', 'wikipedia', 'with', 'wise', '##ly', '-', 'chosen', 'key', '##words', 'to', 'find', 'it', '.', '\"']\n",
      "\n",
      "Token IDs:  [1000, 4851, 5198, 2000, 3967, 2017, 9139, 6203, 3559, 2012, 6391, 1012, 4293, 1012, 14506, 1012, 10715, 1012, 2017, 2024, 2747, 1006, 2004, 2017, 2031, 2589, 1999, 1996, 2627, 1007, 3967, 2075, 5198, 1010, 1999, 3327, 15631, 1010, 4851, 2068, 2000, 1041, 1011, 5653, 2017, 9139, 2055, 2242, 1012, 1045, 2079, 2025, 2113, 2054, 2017, 4299, 2000, 2831, 2055, 2021, 1045, 2064, 14306, 2017, 2008, 2045, 2003, 2763, 1037, 2488, 2126, 2000, 14570, 3649, 2009, 2003, 2008, 2017, 4299, 2000, 14570, 1012, 1999, 3327, 1010, 2065, 2017, 2031, 1037, 3423, 5142, 2055, 2115, 3167, 2592, 2006, 2151, 16948, 3720, 1010, 3531, 3967, 1996, 27178, 2869, 2136, 2004, 2566, 16948, 1024, 1038, 14277, 2098, 4183, 1001, 3423, 1035, 3314, 1012, 2065, 2017, 2024, 2108, 28186, 1999, 2070, 5450, 1010, 3531, 2156, 16948, 1024, 16011, 2005, 6040, 1012, 2065, 2009, 1005, 1055, 1037, 2470, 2622, 1010, 3191, 16948, 1024, 2470, 15680, 1012, 2065, 2009, 1005, 1055, 1037, 9385, 3291, 1010, 3531, 3582, 1996, 7826, 2012, 16948, 1024, 9385, 1035, 13302, 1001, 2592, 1035, 2005, 1035, 9385, 1035, 5608, 1012, 1045, 2064, 1005, 1056, 2428, 2228, 1997, 2205, 2116, 2060, 3471, 2008, 2024, 2488, 8971, 2125, 1011, 15536, 3211, 1998, 9139, 1012, 3531, 2022, 5204, 1997, 2256, 11594, 2114, 1000, 1000, 10683, 7741, 1000, 1000, 1010, 2029, 2003, 15536, 3211, 1011, 21435, 2005, 2667, 2000, 8587, 2490, 2005, 2070, 3327, 8476, 1012, 2065, 2111, 2079, 3967, 2017, 1010, 3531, 3191, 1059, 2361, 1024, 10373, 19894, 2000, 2113, 2256, 11032, 2055, 1996, 9394, 1997, 2037, 22028, 1012, 2022, 5204, 2008, 2011, 14739, 2115, 1041, 1011, 5653, 4769, 1010, 2017, 2024, 3228, 2185, 2115, 2797, 2592, 1012, 1000, 1000, 3972, 20624, 3070, 1000, 1000, 2009, 2013, 5310, 1005, 1055, 2831, 5530, 1010, 2004, 2017, 2562, 4851, 1997, 10195, 1010, 2515, 2025, 2025, 6366, 2008, 2592, 2013, 2270, 3193, 1012, 22267, 1010, 2065, 2009, 1005, 1055, 2242, 4498, 2367, 2084, 1045, 1005, 2310, 11436, 1010, 2045, 1005, 1055, 2145, 2471, 7543, 1037, 2488, 2126, 2000, 5047, 2009, 1012, 3945, 16948, 2007, 7968, 2135, 1011, 4217, 3145, 22104, 2000, 2424, 2009, 1012, 1000]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "sentence_number = 0\n",
    "print('Original: ', sentences_train[sentence_number])\n",
    "print()\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences_train[sentence_number]))\n",
    "print()\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences_train[sentence_number])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация всех предложений в датасете. С помощью encode_plus токенизируем, приводим к определнной длине, получаем маску \"внимания\" (attention_mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_attention_mask_labels(sentences, labels):\n",
    "    input_ids, attention_masks = [], []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoder_dict = tokenizer.encode_plus(\n",
    "        sent, # предложение для токенизации\n",
    "        add_special_tokens=True, # добавляем токен в начало и конец предложения\n",
    "        max_length=64, # ограничиваем максимальную длину выходных токенизированных строк\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True, # возвращаем attention_mask\n",
    "        return_tensors='pt' # получить данные в виде тензоров pytorch\n",
    "      )\n",
    "\n",
    "        input_ids.append(encoder_dict['input_ids'])\n",
    "        attention_masks.append(encoder_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертируем списки в отдельные тензоры PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train, attention_masks_train, labels_train = get_input_attention_mask_labels(sentences_train, labels_train)\n",
    "\n",
    "input_ids_valid, attention_masks_valid, labels_valid = get_input_attention_mask_labels(sentences_valid, labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем данные на обучающую и валидационную выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_valid, attention_masks_valid, labels_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на батчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем модель нейронной сети. Берём предобученную 12 слойную\n",
    "модель BERT без регистра \"bert-base-uncased\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # используем 12 слойную модель BERT без регистра\n",
    "    num_labels=2, # количество выходных слоев для бинарной классификации\n",
    "    output_attentions=False, # не возвращаем веса для attention слоев\n",
    "    output_hidden_states=False # не возвращаем веса для скрытых слоев\n",
    ")\n",
    "\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем оптимизатор для регулирования весов и скорости обучения.\n",
    "Задаем количество эпох и планировщик Learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,\n",
    "                  eps=1e-8\n",
    "                  )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы расчета времени, точности и f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def flat_f1(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно переходить к самому обучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения проверим две модели нейронку BERT и старый добрый подход TF_IDF\n",
    "с моделью CatBoostClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели производилось в Google Colab ссылка\n",
    "на ноутбук с выводами\n",
    "https://colab.research.google.com/drive/1Rfti84o9ujvP_t8ct8Aa-WS3Kw7PZT-J?usp=sharing ниже\n",
    " так же продублирован вывод в результате работы кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(device, model, train_dataloader, optimizer, scheduler):\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    # переводим модель в режим тренировки\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(' Batch {:>5,} of {:>5}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        # извлекаем все компоненты из полученного батча\n",
    "        b_input_ids, b_inpit_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        # очищаем ранее посчитанные градиенты\n",
    "        model.zero_grad()\n",
    "        # выполняем прямой проход по данным\n",
    "        loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_inpit_mask, labels=b_labels)\n",
    "        # накапливаем тренировочную функцию потерь по всем батчам\n",
    "        total_train_loss += loss.item()\n",
    "        # выполняем обратное распространение ошибки, чтобы посчитать градиент\n",
    "        loss.backward()\n",
    "        # ограничиваем максимальный размер градиента до 1.0\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "        # обновляем параметры модели используя рассчитанные градиенты с помощью выбранного оптимизатора и \n",
    "        # текущего learning rate\n",
    "        optimizer.step()\n",
    "        # обновляем learning rate\n",
    "        scheduler.step()\n",
    "    # считаем среднее значение функции потерь\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\" Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\" Training epoch took: {:}\".format(training_time))\n",
    "    return avg_train_loss, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(device, model, validation_dataloader):\n",
    "    # переводим модель в режим evaluation \n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1_score = 0\n",
    "    for batch in validation_dataloader:\n",
    "         # извлекаем все компоненты из полученного батча\n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "         # говорим что нам не нужен вычислительный граф для подсчета градиентов\n",
    "        with torch.no_grad():\n",
    "            # прямой проход по нейросети и получение выходных значений\n",
    "            (loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # накапливаем значения для функции потерь для валидации\n",
    "        total_eval_loss += loss.item()\n",
    "        # переносим значения с GPU на CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # считаем точьность для отдельного батча с текстами и накапливаем значение\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        total_eval_f1_score += flat_f1(logits, label_ids)\n",
    "    \n",
    "    avg_val_f1_score = total_eval_f1_score / len(validation_dataloader)\n",
    "    print(\" F1_score: {0:.2f}\".format(avg_val_f1_score))\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\" Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    # считаем среднюю функцию потерь для всех батчей\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\" Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "    return avg_val_loss, avg_val_accuracy, avg_val_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем есть ли в системе GPU с CUDA если есть используем его, иначе работает на CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "device = get_device()\n",
    "for epoch_i in range(0, epochs):\n",
    "    avg_train_loss, training_time = train_step(device,\n",
    "                                               model,\n",
    "                                               train_dataloader,\n",
    "                                               optimizer,\n",
    "                                               scheduler)\n",
    "    avg_val_loss, avg_val_accuracy, avg_val_f1_score = validation_step(device,\n",
    "                                                     model,\n",
    "                                                     validation_dataloader)\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'Epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Validation Loss': avg_val_loss,\n",
    "            'Validation Accur.': avg_val_accuracy,\n",
    "            'F1 score': avg_val_f1_score,\n",
    "            'Training Time': training_time\n",
    "        }\n",
    "    )\n",
    "print(\"Training complete! Total training took {:} (hh:mm:ss)\".format(format_time((time.time() - total_t0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод BERT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1 GPU(s) available.  \n",
    "We will use the GPU: Tesla T4  \n",
    "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: torch.nn.utils.clip_grad_norm is now   deprecated in favor of torch.nn.utils.clip_grad_norm_.  \n",
    " Batch    40 of    50. Elapsed: 0:00:15.  \n",
    " Average training loss: 0.42  \n",
    " Training epoch took: 0:00:19  \n",
    " F1_score: 0.65  \n",
    " Accuracy: 0.94  \n",
    " Validation Loss: 0.16  \n",
    " Batch    40 of    50. Elapsed: 0:00:15.  \n",
    " Average training loss: 0.18  \n",
    " Training epoch took: 0:00:19  \n",
    " F1_score: 0.66  \n",
    " Accuracy: 0.93  \n",
    " Validation Loss: 0.21  \n",
    " Batch    40 of    50. Elapsed: 0:00:15.  \n",
    " Average training loss: 0.10  \n",
    " Training epoch took: 0:00:19  \n",
    " F1_score: 0.67  \n",
    " Accuracy: 0.93  \n",
    " Validation Loss: 0.21  \n",
    " Batch    40 of    50. Elapsed: 0:00:15.  \n",
    " Average training loss: 0.06  \n",
    " Training epoch took: 0:00:18  \n",
    " F1_score: 0.64  \n",
    " Accuracy: 0.92  \n",
    " Validation Loss: 0.24  \n",
    "Training complete! Total training took 0:01:22 (hh:mm:ss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если проверять на несбалансированной выборке то F1_score всего 0.64, ниже требуемых 0,75, поэтому для интереса я сделал еще по всей выборке (159_571 твитов) без балансировки https://colab.research.google.com/drive/1MhTD2-di5ilMfBh017_TAOR8JLIxSjlY?usp=sharing ниже так же продублирована часть вывода в результате работы кода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Batch 3,680 of  3990. Elapsed: 0:40:28.  \n",
    " Batch 3,720 of  3990. Elapsed: 0:40:54.  \n",
    " Batch 3,760 of  3990. Elapsed: 0:41:21.  \n",
    " Batch 3,800 of  3990. Elapsed: 0:41:47.  \n",
    " Batch 3,840 of  3990. Elapsed: 0:42:13.  \n",
    " Batch 3,880 of  3990. Elapsed: 0:42:40.  \n",
    " Batch 3,920 of  3990. Elapsed: 0:43:06.  \n",
    " Batch 3,960 of  3990. Elapsed: 0:43:33.  \n",
    " Average training loss: 0.02  \n",
    " Training epoch took: 0:43:52  \n",
    " F1_score: 0.77  \n",
    " Accuracy: 0.96  \n",
    " Validation Loss: 0.22  \n",
    "Training complete! Total training took 3:10:34 (hh:mm:ss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность F1_score: 0.77 при общем времени на 4 эпохи 3часа 10минут 34секунды по всей выборке в 159_571 твитов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим все необходимы для обработки текста библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "nltk.download('stopwords')\n",
    "m = Mystem()\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(row):\n",
    "    text = row['text']\n",
    "    text_re = ' '.join(re.sub(\"[^a-zA-Z ]\", \" \", text).lower().split())\n",
    "    lemmas = m.lemmatize(text_re)\n",
    "    row['lemma_text'] = ' '.join([token for token in lemmas\n",
    "                       if token not in ['\\n',' \\n'] and\n",
    "                       len(token) > 1 and\n",
    "                       token not in stopwords and\n",
    "                       token.strip() not in punctuation])\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же как и для модели BERT возьмем 1600 сбалансированных твитов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_train_valid(df, count=2000, size=0.8):\n",
    "    size_train = int(count*size)\n",
    "    size_valid = count - size_train\n",
    "    df_train = get_balanced_dataframe(count=size_train)\n",
    "    print('Всего негативных комментариев в тренировочной', df_train.toxic.sum())\n",
    "    unbalanced_df = df[(~df.index.isin(df_train.index))]\n",
    "    df_valid = unbalanced_df.sample(size_valid)\n",
    "    return df_train, df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим и лемматизируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер изначального датасета (159571, 2)\n",
      "Размер подвыборки (1600, 2)\n",
      "10 случайных примеров из датасета                                                      text  toxic\n",
      "122112  \"\\nOK. I was about to say I had no clue what w...      0\n",
      "99195   Suck my dikkkkk ==\\nSuck my dikkkkk Gogo Gogo ...      1\n",
      "33935   \"WARNING: MrZaius has a hard-on for the term \"...      1\n",
      "110749  Doesn't matter  you are still using it for pur...      0\n",
      "114385  \"\\n\\n Thanks \\n\\n  The Random Acts of Kindness...      0\n",
      "76164   Bloody asshole \\n\\nYou are a bloody asshole an...      1\n",
      "82182       AM A GAY FAG THAT FUCKS MY MOM IN HER FAT ASS      1\n",
      "110078  The inhabitants of Istanbul call the city Ista...      0\n",
      "136467  Go take a long walk off a short pier. \\n\\nSee ...      1\n",
      "38033                Go fuck yourself you lifeless pedant      1\n",
      "Всего негативных комментариев в тренировочной 800\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df_train, df_valid = get_df_train_valid(df, count=2000, size=0.8)\n",
    "df_train = df_train.apply(preprocessing_text, axis=1)\n",
    "df_valid = df_valid.apply(preprocessing_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель CatBoostClassifier на 2_000 примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.019138\n",
      "0:\tlearn: 0.6874258\ttotal: 118ms\tremaining: 1m 57s\n",
      "100:\tlearn: 0.5205111\ttotal: 14.5s\tremaining: 2m 8s\n",
      "200:\tlearn: 0.4669611\ttotal: 29.4s\tremaining: 1m 56s\n",
      "300:\tlearn: 0.4283921\ttotal: 44.3s\tremaining: 1m 42s\n",
      "400:\tlearn: 0.3912648\ttotal: 59.1s\tremaining: 1m 28s\n",
      "500:\tlearn: 0.3507106\ttotal: 1m 13s\tremaining: 1m 13s\n",
      "600:\tlearn: 0.3173272\ttotal: 1m 28s\tremaining: 58.5s\n",
      "700:\tlearn: 0.2909398\ttotal: 1m 42s\tremaining: 43.6s\n",
      "800:\tlearn: 0.2695489\ttotal: 1m 56s\tremaining: 29s\n",
      "900:\tlearn: 0.2510792\ttotal: 2m 11s\tremaining: 14.4s\n",
      "999:\tlearn: 0.2352781\ttotal: 2m 25s\tremaining: 0us\n",
      "0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "def get_predict_by_tf_idf(df_train, df_valid):\n",
    "    target_train = df_train['toxic']\n",
    "    features_train = df_train.drop(['toxic','text'], axis=1)\n",
    "    \n",
    "    target_valid = df_valid['toxic']\n",
    "    features_valid = df_valid.drop(['toxic','text'], axis=1)\n",
    "\n",
    "\n",
    "    corpus_train = features_train['lemma_text'].values.astype('U')\n",
    "    corpus_valid = features_valid['lemma_text'].values.astype('U')\n",
    "\n",
    "    count_tf_idf_train = TfidfVectorizer(stop_words=stopwords)\n",
    "    tf_idf_train = count_tf_idf_train.fit_transform(corpus_train)\n",
    "\n",
    "    model = CatBoostClassifier(verbose=100)\n",
    "    model.fit(tf_idf_train, target_train)\n",
    "\n",
    "    tf_idf_test = count_tf_idf_train.transform(corpus_valid)\n",
    "\n",
    "    pred_valid = model.predict(tf_idf_test)\n",
    "    print(f1_score(target_valid, pred_valid))\n",
    "    \n",
    "get_predict_by_tf_idf(df_train, df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод TF_IDF (при 2_000 примеров):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate set to 0.019138   \n",
    "0:\tlearn: 0.6874258\ttotal: 118ms\tremaining: 1m 57s  \n",
    "100:\tlearn: 0.5205111\ttotal: 14.5s\tremaining: 2m 8s  \n",
    "200:\tlearn: 0.4669611\ttotal: 29.4s\tremaining: 1m 56s  \n",
    "300:\tlearn: 0.4283921\ttotal: 44.3s\tremaining: 1m 42s  \n",
    "400:\tlearn: 0.3912648\ttotal: 59.1s\tremaining: 1m 28s  \n",
    "500:\tlearn: 0.3507106\ttotal: 1m 13s\tremaining: 1m 13s  \n",
    "600:\tlearn: 0.3173272\ttotal: 1m 28s\tremaining: 58.5s  \n",
    "700:\tlearn: 0.2909398\ttotal: 1m 42s\tremaining: 43.6s  \n",
    "800:\tlearn: 0.2695489\ttotal: 1m 56s\tremaining: 29s  \n",
    "900:\tlearn: 0.2510792\ttotal: 2m 11s\tremaining: 14.4s  \n",
    "999:\tlearn: 0.2352781\ttotal: 2m 25s\tremaining: 0us  \n",
    "0.5416666666666666   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1_score = 0.5417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем 10_000 примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер изначального датасета (159571, 2)\n",
      "Размер подвыборки (8000, 2)\n",
      "10 случайных примеров из датасета                                                      text  toxic\n",
      "70374   \"\\nNational Foundation Day is an \"\"ancient his...      0\n",
      "35109   butt butt butt butt butt butt butt butt butt b...      1\n",
      "115364  template:user atheist and zen buddhist \\nappar...      0\n",
      "121467  \"\\n\\n Always complaining... \\n\\nYou know, nigg...      1\n",
      "84709   Peacemaker, how evil you must be when you now ...      1\n",
      "81651   - Bridgnorth.jpg|left|thumb|]]\\n\\nBridgnorth M...      0\n",
      "49745   \"\\n\\n Listen up fagtard \\n\\nHow can the fact t...      1\n",
      "45746   \"\\n\\n User: Jhoney123 \\n\\nI feel Jhoney123 sho...      1\n",
      "37756   Who think you are ????  You the worst offender...      1\n",
      "56289      I'm a fat pig.  Does that belong on Wikipedia?      1\n",
      "Всего негативных комментариев в тренировочной 4000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df_train, df_valid = get_df_train_valid(df, count=10000, size=0.8)\n",
    "df_train = df_train.apply(preprocessing_text, axis=1)\n",
    "df_valid = df_valid.apply(preprocessing_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель CatBoostClassifier на 10_000 примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.030179\n",
      "0:\tlearn: 0.6813042\ttotal: 692ms\tremaining: 11m 31s\n",
      "100:\tlearn: 0.4658948\ttotal: 53.6s\tremaining: 7m 57s\n",
      "200:\tlearn: 0.4130504\ttotal: 1m 45s\tremaining: 7m\n",
      "300:\tlearn: 0.3782224\ttotal: 2m 37s\tremaining: 6m 6s\n",
      "400:\tlearn: 0.3444523\ttotal: 3m 29s\tremaining: 5m 13s\n",
      "500:\tlearn: 0.3184733\ttotal: 4m 21s\tremaining: 4m 20s\n",
      "600:\tlearn: 0.2968444\ttotal: 5m 15s\tremaining: 3m 29s\n",
      "700:\tlearn: 0.2787255\ttotal: 6m 7s\tremaining: 2m 36s\n",
      "800:\tlearn: 0.2635980\ttotal: 6m 59s\tremaining: 1m 44s\n",
      "900:\tlearn: 0.2516716\ttotal: 7m 51s\tremaining: 51.8s\n",
      "999:\tlearn: 0.2401656\ttotal: 8m 43s\tremaining: 0us\n",
      "0.6584766584766585\n"
     ]
    }
   ],
   "source": [
    "get_predict_by_tf_idf(df_train, df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод TF_IDF (при 10_000 примеров):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate set to 0.030179  \n",
    "0:\tlearn: 0.6813042\ttotal: 692ms\tremaining: 11m 31s  \n",
    "100:\tlearn: 0.4658948\ttotal: 53.6s\tremaining: 7m 57s  \n",
    "200:\tlearn: 0.4130504\ttotal: 1m 45s\tremaining: 7m  \n",
    "300:\tlearn: 0.3782224\ttotal: 2m 37s\tremaining: 6m 6s  \n",
    "400:\tlearn: 0.3444523\ttotal: 3m 29s\tremaining: 5m 13s  \n",
    "500:\tlearn: 0.3184733\ttotal: 4m 21s\tremaining: 4m 20s  \n",
    "600:\tlearn: 0.2968444\ttotal: 5m 15s\tremaining: 3m 29s  \n",
    "700:\tlearn: 0.2787255\ttotal: 6m 7s\tremaining: 2m 36s  \n",
    "800:\tlearn: 0.2635980\ttotal: 6m 59s\tremaining: 1m 44s  \n",
    "900:\tlearn: 0.2516716\ttotal: 7m 51s\tremaining: 51.8s  \n",
    "999:\tlearn: 0.2401656\ttotal: 8m 43s\tremaining: 0us  \n",
    "0.6584766584766585    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1_score = 0.6585"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же как и с BERT сделана методика\n",
    "TF_IDF c CatBoostClassifier по всей (несбалансированной)\n",
    " выборке, но уже в DataShpere аналог Colab, но от Яндекса.\n",
    " Ниже вывод в результате выполнения и время(32 ядра и 4 Tesla V100):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate set to 0.081698  \n",
    "0:\tlearn: 0.6113123\ttotal: 976ms\tremaining: 16m 15s  \n",
    "100:\tlearn: 0.1779025\ttotal: 1m 6s\tremaining: 9m 52s  \n",
    "200:\tlearn: 0.1544423\ttotal: 2m 16s\tremaining: 9m 2s  \n",
    "300:\tlearn: 0.1418005\ttotal: 3m 28s\tremaining: 8m 3s  \n",
    "400:\tlearn: 0.1331876\ttotal: 4m 42s\tremaining: 7m 2s  \n",
    "500:\tlearn: 0.1261722\ttotal: 5m 57s\tremaining: 5m 55s  \n",
    "600:\tlearn: 0.1208737\ttotal: 7m 12s\tremaining: 4m 47s  \n",
    "700:\tlearn: 0.1166288\ttotal: 8m 27s\tremaining: 3m 36s  \n",
    "800:\tlearn: 0.1127415\ttotal: 9m 43s\tremaining: 2m 25s  \n",
    "900:\tlearn: 0.1095175\ttotal: 10m 59s\tremaining: 1m 12s  \n",
    "999:\tlearn: 0.1062234\ttotal: 12m 15s\tremaining: 0us  \n",
    "0.7559168925022584  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Модель                   | F1-score   | Примеров train |  Примеров valid | max_length_text | batch_size |\n",
    "| :------------------------|-----------:|---------------:|----------------:|----------------:|-----------:|\n",
    "| bert-base-uncased 2000   |     0.6400 |          1_600 |             400 |             64  |        32  | \n",
    "| bert-base-uncased 10000  |     0.6200 |          8_000 |           2_000 |             64  |        32  |\n",
    "| bert-base-uncased all    | **0.7700** |        127_657 |          31_914 |             64  |        32  |\n",
    "| CatBoostClassifier 2000  |     0.5417 |          1_600 |             400 |              -  |         -  |\n",
    "| CatBoostClassifier 10000 | **0.6585** |          8_000 |           2_000 |              -  |         -  |\n",
    "| CatBoostClassifier all   |     0.7559 |        127_657 |          31_914 |              -  |         -  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В этой работе у нас был датасет из 159_571 твитов, из них 16_225 (10%)\n",
    "негативные и 143_346 позитивные. Из несбалансированной выборки сделали\n",
    " сбалансированную, взяли всего 1600 твитов по 800 позитивных и негативных.\n",
    " Обучал на 400 случайных.\n",
    "  \n",
    "Разделили выборку на тренировочную 80% и валидационную 20%. Провели\n",
    "токенизацию, взяли максимальную длину в 64 токена. Разделили данные\n",
    "на батчи размером 32. Взяли предобученную 12 слойную модель BERT без\n",
    " регистра \"bert-base-uncased\". Обучали 4 эпохи как рекомендуют создатели BERT.\n",
    "  \n",
    "В результате F1_score последней эпохи равно 0.64, за полторы минуты (01:22).\n",
    "Что показывает не очень хорошую точность. Датасет из 10_000 дал похожие\n",
    " результаты (поэтому его я сюда не выводил, обучал в колабе).\n",
    "  \n",
    "Так же был применен подход TF_IDF, взято 2_000, 10_000 сбалансированных\n",
    "примеров, лематизированно, а затем передано в CatBoostClassifier.\n",
    "На 2_000 получил F1_score=0.5417, что ниже заданной метрики (0.75),\n",
    "но обучалась модель около 10 минут, что дольше чем BERT и результат похуже.\n",
    " На 10_000 примеров CatBoost обучался десятки минут, с точностью F1_score=0.6585,\n",
    "  что уже близко к BERT.\n",
    "  \n",
    "Если учить вообще без изначальной балансировки и по всем параметрам\n",
    "то точность BERT 0.77, а CatBoostClassifier 0,76 и по скорости 3,5 часа\n",
    " и десятки минут, но опять же на разных мощностях, поэтому время сравнивать\n",
    " не совсем корректно. Нельзя сказать однозначно кто лидирует, модель на основе\n",
    "  нейронной 12 слойной сети BERT без регистра \"bert-base-uncased\" или\n",
    "   CatBoostClassifier, модели показали одинаково плохой результат, хоть\n",
    "   он и дотягивает до требуемых 0,75, но учитывая разбаланс(90% и 10%)\n",
    "   если предсказывать всем твитам положительную метку у нас будет точность\n",
    "   90%. Но бустинг CatBoostClassifier показал сопоставимый по точности\n",
    "   результат с нейронкой BERT.\n",
    "\n",
    "Из плюсов использования CatBoostClassifier - его гораздо проще конфигурировать,\n",
    "он показал неплохой результат \"из коробки\" даже без настройки, и намного\n",
    " удобнее готовить данные для обучения."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}